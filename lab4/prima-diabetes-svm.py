# Training the model with different kernels and then printing the accuracy score for each kernel.
"""diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pH2PBA8xNirOIqM1mejlqrERqYgckpJb?usp=share_link

to run it at google enviroment, remember to upload dataset csv file
"""

"""
Marcin Biedrzycki s13448
Tomasz Sosiński s16216
"""


import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn import metrics
from sklearn import svm
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from tensorflow.keras import models, layers
from tensorflow.keras.callbacks import EarlyStopping

# Reading the csv file and storing it in a dataframe.
df = pd.read_csv('diabetes.csv')
# Showing the first 5 rows of the dataframe.
df.head()

# Showing the data types of the columns in the dataframe.
df.info()

plt.figure(figsize=(10, 6))

# Creating a heatmap of the correlation between the variables in the dataframe.
heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='viridis')

# Creating a heatmap of the correlation between the variables in the dataframe.
heatmap.set_title('Correlation Heatmap', fontdict={'fontsize': 12}, pad=12)
plt.show()

# Creating a figure with 7 rows and 1 column.
fig, ax = plt.subplots(7, 1, figsize=(10, 45))

# Creating a histogram for each of the variables in the dataframe.
sns.histplot(ax=ax[0], data=df, x='Age', hue='Outcome', kde=True)
sns.histplot(ax=ax[1], data=df, x='Glucose', hue='Outcome', kde=True)
sns.histplot(ax=ax[2], data=df, x='Insulin', hue='Outcome', kde=True)
sns.histplot(ax=ax[3], data=df, x='BMI', hue='Outcome', kde=True)
sns.histplot(ax=ax[4], data=df, x='BloodPressure', hue='Outcome', kde=True)
sns.histplot(ax=ax[5], data=df, x='SkinThickness', hue='Outcome', kde=True)
sns.histplot(ax=ax[6], data=df, x='DiabetesPedigreeFunction', hue='Outcome', kde=True)

# Showing the plot.
plt.show()

# Splitting the data into training and testing sets.
X_tr, X_ts, Y_tr, Y_ts = train_test_split(df.drop(columns='Outcome'), df['Outcome'],
                                          test_size=0.15, shuffle=True, random_state=42, stratify=df['Outcome'])

# Scaling the data.
scaler1 = MinMaxScaler()
scaler2 = StandardScaler()
X_tr[['Age', 'Insulin', 'DiabetesPedigreeFunction']] = scaler1.fit_transform(
    X_tr[['Age', 'Insulin', 'DiabetesPedigreeFunction']])
X_ts[['Age', 'Insulin', 'DiabetesPedigreeFunction']] = scaler1.transform(
    X_ts[['Age', 'Insulin', 'DiabetesPedigreeFunction']])

X_tr[['Glucose', 'BMI', 'BloodPressure', 'SkinThickness']] = scaler2.fit_transform(
    X_tr[['Glucose', 'BMI', 'BloodPressure', 'SkinThickness']])
X_ts[['Glucose', 'BMI', 'BloodPressure', 'SkinThickness']] = scaler2.transform(
    X_ts[['Glucose', 'BMI', 'BloodPressure', 'SkinThickness']])

# Creating a sequential model.
model = models.Sequential()

# Adding a layer to the model.
model.add(layers.Dense(20, activation='relu', input_shape=(X_tr.shape[1],)))
model.add(layers.Dense(1, activation='sigmoid'))

# Compiling the model.
model.compile(loss='binary_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])

# Stopping the training process if the validation loss does not improve for 10 epochs.
es = EarlyStopping(monitor='val_loss', mode='min', patience=10, verbose=1)

# Fitting the model to the training data.
history = model.fit(X_tr, Y_tr,
                    epochs=75,
                    batch_size=15,
                    validation_data=(X_ts, Y_ts),
                    callbacks=[es])

# Adding a title to the figure.
fig.suptitle('Train evaluation')

# Plotting the accuracy of the model on the training and validation sets.
sns.lineplot(ax=ax[0], x=np.arange(0, len(history.history['accuracy'])), y=history.history['accuracy'])
sns.lineplot(ax=ax[0], x=np.arange(0, len(history.history['accuracy'])), y=history.history['val_accuracy'])

# Plotting the accuracy of the model on the training and validation sets.
ax[0].legend(['Train', 'Validation'])
ax[0].set_title('Accuracy')

# Plotting the loss of the model on the training and validation sets.
sns.lineplot(ax=ax[1], x=np.arange(0, len(history.history['loss'])), y=history.history['loss'])
sns.lineplot(ax=ax[1], x=np.arange(0, len(history.history['loss'])), y=history.history['val_loss'])

# Plotting the loss of the model on the training and validation sets.
ax[1].legend(['Train', 'Validation'])
ax[1].set_title('Loss')


# Showing the plot.
plt.show()

# Predicting the outcome of the test set.
Y_pred = model.predict(X_ts)

error = []
# Creating a grid of 10000 values between the minimum and maximum values of the predictions.
grid = np.linspace(min(Y_pred)[0], max(Y_pred)[0], 10000)

# Calculating the error for each value of the grid.
for i in grid:
    y_pred_i = (Y_pred > i).astype('int32')
    fails = confusion_matrix(Y_ts, y_pred_i)[0, 1] + confusion_matrix(Y_ts, y_pred_i)[1, 0]
    error_i = fails / len(Y_ts)
    error.append(error_i)

i_error = list(zip(grid, error))
i_min = i_error[np.argmin(error)][0]

# Plotting the error by cutoff.
plt.figure(figsize=(12, 8))
griderror = sns.lineplot(x=grid, y=error)
griderror.set_title('Error by cutoff')
griderror.set_xlabel('cutoff')
griderror.set_ylabel('error')
plt.scatter(x=i_min, y=min(error), s=300, color='r', marker='+', alpha=0.3)
plt.show()

# Predicting the outcome of the test set.
Y_pred = (Y_pred > i_min).astype('int32')
print('Y_pred > {:.4f}, means positive'.format(i_min))

# Creating a confusion matrix.
conf_mat = confusion_matrix(Y_ts, Y_pred)
sns.set_style(style='dark')
plt.figure(figsize=(12, 8))
heatmap = sns.heatmap(conf_mat, vmin=np.min(conf_mat.all()), vmax=np.max(conf_mat), annot=True, fmt='d',
                      annot_kws={"fontsize": 20}, cmap='Spectral')
heatmap.set_title('Confusion Matrix Heatmap\n¿Is diabetic?', fontdict={'fontsize': 15}, pad=12)
heatmap.set_xlabel('Predicted', fontdict={'fontsize': 14})
heatmap.set_ylabel('Actual', fontdict={'fontsize': 14})
heatmap.set_xticklabels(['NO', 'YES'], fontdict={'fontsize': 12})
heatmap.set_yticklabels(['NO', 'YES'], fontdict={'fontsize': 12})
plt.show()

print('-Acuracy achieved: {:.2f}%\n-Accuracy by model was: {:.2f}%\n-Accuracy by validation was: {:.2f}%'.
      format((1 - min(error)) * 100, (history.history['accuracy'][-1]) * 100,
             (history.history['val_accuracy'][-1]) * 100))

# Splitting the data into training and testing sets.
X = df.drop(['Outcome'], axis=1)
y = df['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)


# The above code is using the SVM model to predict the outcome of the test data.
model = svm.SVC(C=2, gamma='scale', kernel='linear')
model.fit(X_train, y_train)
prediction = model.predict(X_test)
print(metrics.accuracy_score(prediction, y_test) * 100)
print(confusion_matrix(y_test, prediction))
print(classification_report(y_test, prediction))
print('MAE:', metrics.mean_absolute_error(y_test, prediction))
print('MSE:', metrics.mean_squared_error(y_test, prediction))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))


classifier = svm.SVC(kernel ='rbf', random_state = 0)
 # training set in x, y axis
classifier.fit(X_train, y_train)

classifierSig = svm.SVC(kernel ='sigmoid')
classifierSig.fit(X_train, y_train)

# Training the model with different kernels and then printing the accuracy score for each kernel.
for k in ('linear', 'poly', 'rbf', 'sigmoid'):
    model = svm.SVC(kernel=k)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_train)
    print(k)
    print(metrics.accuracy_score(y_train, y_pred))


